"""
Simple Textbook RAG System with SQLite and Claude

This system:
1. Extracts text from PDFs (textbooks)
2. Intelligently splits by sections/chapters
3. Stores in SQLite with metadata
4. Uses BM25 search for retrieval
5. Generates responses with Claude
"""

import os
import sqlite3
import json
import re
import uuid
from typing import List, Dict, Any

# PDF processing
from pypdf import PdfReader

# Text processing
from rank_bm25 import BM25Okapi
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

# Claude integration
from langchain.llms import Anthropic
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# ---------------------- Text Extraction ----------------------

def extract_text_from_pdf(pdf_path: str) -> Dict[int, str]:
    """Extract text content from a PDF file, page by page."""
    try:
        reader = PdfReader(pdf_path)
        pages = {}
        for i, page in enumerate(reader.pages):
            page_text = page.extract_text()
            if page_text:
                pages[i] = page_text
        return pages
    except Exception as e:
        print(f"Error extracting text from {pdf_path}: {e}")
        return {}

def detect_section_headers(text: str) -> List[tuple]:
    """
    Detect potential section headers in text.
    Returns a list of (start_pos, header_text) tuples.
    """
    # Common textbook section patterns
    patterns = [
        r'^Chapter\s+\d+[\.\:]\s+.+$',  # Chapter headers
        r'^\d+\.\d+\s+.+$',             # Section headers like 1.2 Topic
        r'^Section\s+\d+[\.\:]\s+.+$',  # Section headers
        r'^[A-Z][A-Z\s]+$',             # ALL CAPS headers
    ]
    
    headers = []
    for pattern in patterns:
        for match in re.finditer(pattern, text, re.MULTILINE):
            headers.append((match.start(), match.group()))
    
    return sorted(headers)

def split_by_sections(pages: Dict[int, str]) -> List[Dict[str, Any]]:
    """
    Split text into sections based on detected headers.
    Returns a list of section dictionaries with header, content, and page info.
    """
    sections = []
    current_section = {"header": "Introduction", "content": "", "start_page": 0}
    
    for page_num, page_text in sorted(pages.items()):
        # If this is the first page, set it as the start page
        if not sections and not current_section["content"]:
            current_section["start_page"] = page_num
        
        headers = detect_section_headers(page_text)
        
        if not headers:
            # No headers on this page, add to current section
            current_section["content"] += f"\n\n{page_text}"
        else:
            # For each header on the page
            last_pos = 0
            for pos, header_text in headers:
                # Add content before this header to current section
                if pos > 0:
                    current_section["content"] += f"\n\n{page_text[last_pos:pos]}"
                
                # Save current section if it has content
                if current_section["content"].strip():
                    sections.append(current_section)
                
                # Start a new section
                current_section = {
                    "header": header_text,
                    "content": "",
                    "start_page": page_num
                }
                
                last_pos = pos + len(header_text)
            
            # Add remaining content after the last header
            if last_pos < len(page_text):
                current_section["content"] += page_text[last_pos:]
    
    # Add the final section if it has content
    if current_section["content"].strip():
        sections.append(current_section)
    
    return sections

# ---------------------- Database Setup ----------------------

def setup_sqlite_db(db_path: str = "textbook_rag.db") -> sqlite3.Connection:
    """Initialize SQLite database for the RAG system."""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Create textbooks table
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS textbooks (
        id TEXT PRIMARY KEY,
        title TEXT,
        file_path TEXT,
        add_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    ''')
    
    # Create sections table
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS sections (
        id TEXT PRIMARY KEY,
        textbook_id TEXT,
        header TEXT,
        content TEXT,
        start_page INTEGER,
        section_number INTEGER,
        tokens TEXT,
        FOREIGN KEY (textbook_id) REFERENCES textbooks(id)
    )
    ''')
    
    conn.commit()
    return conn

# ---------------------- Search Engine ----------------------

class TextbookSearchEngine:
    def __init__(self, db_path: str = "textbook_rag.db"):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row
        
        # BM25 index cache for each textbook
        self.bm25_indices = {}
        self.section_data = {}
    
    def _load_textbook_sections(self, textbook_id: str = None):
        """Load sections for the specified textbook or all textbooks."""
        cursor = self.conn.cursor()
        
        if textbook_id:
            cursor.execute(
                "SELECT id, textbook_id, header, content, tokens FROM sections WHERE textbook_id = ?",
                (textbook_id,)
            )
        else:
            cursor.execute(
                "SELECT id, textbook_id, header, content, tokens FROM sections"
            )
        
        sections = cursor.fetchall()
        
        # Group sections by textbook
        textbook_sections = {}
        section_data = {}
        
        for section in sections:
            section_id = section['id']
            t_id = section['textbook_id']
            
            # Convert tokens back from JSON string
            tokens = json.loads(section['tokens']) if section['tokens'] else word_tokenize(section['content'].lower())
            
            if t_id not in textbook_sections:
                textbook_sections[t_id] = []
            
            textbook_sections[t_id].append(tokens)
            
            # Store the original section data
            section_data[section_id] = {
                'textbook_id': t_id,
                'header': section['header'],
                'content': section['content'],
                'tokens': tokens
            }
        
        # Create BM25 indices for each textbook
        for t_id, sections_tokens in textbook_sections.items():
            self.bm25_indices[t_id] = BM25Okapi(sections_tokens)
        
        self.section_data = section_data
    
    def search(self, query: str, textbook_id: str = None, top_k: int = 5):
        """
        Search for relevant sections based on the query.
        If textbook_id is provided, search only in that textbook.
        """
        # Refresh our indices
        self._load_textbook_sections(textbook_id)
        
        # Tokenize the query
        query_tokens = word_tokenize(query.lower())
        
        results = []
        
        # If specific textbook requested
        if textbook_id and textbook_id in self.bm25_indices:
            # Get scores for this textbook
            scores = self.bm25_indices[textbook_id].get_scores(query_tokens)
            
            # Match scores with section IDs
            cursor = self.conn.cursor()
            cursor.execute(
                "SELECT id FROM sections WHERE textbook_id = ? ORDER BY section_number",
                (textbook_id,)
            )
            section_ids = [row[0] for row in cursor.fetchall()]
            
            # Create result objects
            for i, section_id in enumerate(section_ids):
                if i < len(scores):  # Safety check
                    results.append({
                        'section_id': section_id,
                        'score': scores[i],
                        'textbook_id': textbook_id,
                        'header': self.section_data[section_id]['header'],
                        'content': self.section_data[section_id]['content']
                    })
        else:
            # Search across all textbooks
            for t_id, bm25_index in self.bm25_indices.items():
                # Get scores for this textbook
                scores = bm25_index.get_scores(query_tokens)
                
                # Match scores with section IDs
                cursor = self.conn.cursor()
                cursor.execute(
                    "SELECT id FROM sections WHERE textbook_id = ? ORDER BY section_number",
                    (t_id,)
                )
                section_ids = [row[0] for row in cursor.fetchall()]
                
                # Create result objects
                for i, section_id in enumerate(section_ids):
                    if i < len(scores):  # Safety check
                        results.append({
                            'section_id': section_id,
                            'score': scores[i],
                            'textbook_id': t_id,
                            'header': self.section_data[section_id]['header'],
                            'content': self.section_data[section_id]['content']
                        })
        
        # Sort by score and return top_k
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]

# ---------------------- Document Processing Pipeline ----------------------

class TextbookProcessor:
    def __init__(self, db_path: str = "textbook_rag.db"):
        self.db_conn = setup_sqlite_db(db_path)
    
    def process_textbook(self, pdf_path: str, title: str = None) -> str:
        """Process a textbook PDF and store its content in the database."""
        # Generate textbook ID and use filename as title if not provided
        textbook_id = str(uuid.uuid4())
        if title is None:
            title = os.path.basename(pdf_path).replace('.pdf', '')
        
        # Extract text by pages
        pages = extract_text_from_pdf(pdf_path)
        if not pages:
            return f"Failed to extract text from {title}"
        
        # Split into sections
        sections = split_by_sections(pages)
        
        # Store textbook metadata
        cursor = self.db_conn.cursor()
        cursor.execute(
            "INSERT INTO textbooks (id, title, file_path) VALUES (?, ?, ?)",
            (textbook_id, title, pdf_path)
        )
        
        # Store sections
        for i, section in enumerate(sections):
            section_id = str(uuid.uuid4())
            
            # Create tokens for search
            tokens = word_tokenize(section["content"].lower())
            tokens_json = json.dumps(tokens)
            
            cursor.execute(
                """
                INSERT INTO sections 
                (id, textbook_id, header, content, start_page, section_number, tokens) 
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    section_id, 
                    textbook_id, 
                    section["header"], 
                    section["content"], 
                    section["start_page"],
                    i,
                    tokens_json
                )
            )
        
        self.db_conn.commit()
        
        return f"Successfully processed {title} (ID: {textbook_id}) - {len(sections)} sections created"

# ---------------------- RAG Engine ----------------------

class TextbookRAG:
    def __init__(self, db_path: str = "textbook_rag.db"):
        self.search_engine = TextbookSearchEngine(db_path)
        
        # Initialize Claude
        self.llm = Anthropic(
            model="claude-3-haiku-20240307",
            temperature=0.2,
            max_tokens=1000
        )
    
    def answer_question(self, question: str, textbook_id: str = None, top_k: int = 3):
        """Generate an answer to a question using retrieved textbook sections."""
        # Retrieve relevant sections
        search_results = self.search_engine.search(question, textbook_id, top_k)
        
        if not search_results:
            return "I couldn't find any relevant information to answer your question."
        
        # Create context from search results
        context = "\n\n".join([
            f"SECTION: {result['header']}\n{result['content']}"
            for result in search_results
        ])
        
        # Define the prompt template
        prompt_template = """
        You are a helpful teaching assistant that answers questions based on textbook content.
        
        Below are relevant sections from the textbook:
        
        {context}
        
        Question: {question}
        
        Instructions:
        1. Answer the question based ONLY on the provided textbook sections.
        2. If the provided sections don't contain enough information, say so clearly.
        3. Refer to specific concepts from the textbook sections.
        4. Use an educational, helpful tone appropriate for explaining textbook material.
        5. If mathematical equations or formulas are involved, explain them clearly.
        
        Answer:
        """
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Create the LLM chain
        chain = LLMChain(llm=self.llm, prompt=prompt)
        
        # Run the chain
        response = chain.run(context=context, question=question)
        
        # Include sources in the response
        sources = [f"- {result['header']} (from textbook {result['textbook_id']})" 
                 for result in search_results]
        
        full_response = {
            "answer": response,
            "sources": sources
        }
        
        return full_response

# ---------------------- Usage Example ----------------------

def main():
    # Set up the processor
    textbook_processor = TextbookProcessor()
    
    # Process a textbook
    result = textbook_processor.process_textbook("physics_textbook.pdf", "University Physics")
    print(result)
    
    # Create the RAG engine
    rag_engine = TextbookRAG()
    
    # Answer a question
    question = "Explain Newton's Second Law and its applications"
    answer = rag_engine.answer_question(question)
    
    print(f"\nQuestion: {question}\n")
    print(f"Answer: {answer['answer']}\n")
    print("Sources:")
    for source in answer['sources']:
        print(source)

if __name__ == "__main__":
    main()
